{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport glob\nimport time\nimport numpy as np\nfrom PIL import Image\nfrom pathlib import Path\nfrom tqdm.notebook import tqdm\nimport matplotlib.pyplot as plt\nfrom skimage.color import rgb2lab, lab2rgb\nimport torch\nfrom torch import nn, optim\nfrom torchvision import transforms\nfrom torchvision.utils import make_grid\nfrom torch.utils.data import Dataset, DataLoader\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nimport smtplib","metadata":{"id":"eSsigazmtCUx","execution":{"iopub.status.busy":"2022-06-19T11:53:13.043265Z","iopub.execute_input":"2022-06-19T11:53:13.043649Z","iopub.status.idle":"2022-06-19T11:53:13.050787Z","shell.execute_reply.started":"2022-06-19T11:53:13.043617Z","shell.execute_reply":"2022-06-19T11:53:13.049873Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"from fastai.vision.learner import create_body\nfrom torchvision.models.resnet import resnet18\nfrom fastai.vision.models.unet import DynamicUnet\nfrom fastai.data.external import untar_data, URLs\ncoco_path = untar_data(URLs.COCO_SAMPLE)\ncoco_path = str(coco_path) + \"/train_sample\"\npaths = glob.glob(coco_path + \"/*.jpg\") \nnp.random.seed(123)\npaths_subset = np.random.choice(paths, 10_000, replace=False) \nrand_idxs = np.random.permutation(10_000)\ntrain_idxs = rand_idxs[:8000] \nval_idxs = rand_idxs[8000:]\ntrain_paths = paths_subset[train_idxs]\nval_paths = paths_subset[val_idxs]\nprint(len(train_paths), len(val_paths))","metadata":{"id":"rVB-pdigKtNu","outputId":"c9ddd4ca-134d-4f1c-d793-0b31874abf2f","execution":{"iopub.status.busy":"2022-06-19T11:53:13.057436Z","iopub.execute_input":"2022-06-19T11:53:13.058260Z","iopub.status.idle":"2022-06-19T11:53:13.165387Z","shell.execute_reply.started":"2022-06-19T11:53:13.058233Z","shell.execute_reply":"2022-06-19T11:53:13.164545Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"_, axes = plt.subplots(4, 4, figsize=(10, 10))\nfor ax, img_path in zip(axes.flatten(), train_paths):\n    ax.imshow(Image.open(img_path))\n    ax.axis(\"off\")","metadata":{"id":"I5V1XMn3uBMf","outputId":"360ae901-5c05-4739-86fa-b9cc623ad511","execution":{"iopub.status.busy":"2022-06-19T11:53:13.167073Z","iopub.execute_input":"2022-06-19T11:53:13.167428Z","iopub.status.idle":"2022-06-19T11:53:14.464913Z","shell.execute_reply.started":"2022-06-19T11:53:13.167392Z","shell.execute_reply":"2022-06-19T11:53:14.463374Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"SIZE = 256\nclass ColorizationDataset(Dataset):\n    def __init__(self, paths, split='train'):\n        if split == 'train':\n            self.transforms = transforms.Compose([\n                transforms.Resize((SIZE, SIZE),  Image.BICUBIC),\n                transforms.RandomHorizontalFlip(), # A little data augmentation!\n            ])\n        elif split == 'val':\n            self.transforms = transforms.Resize((SIZE, SIZE),  Image.BICUBIC)\n        \n        self.split = split\n        self.size = SIZE\n        self.paths = paths\n    \n    def __getitem__(self, idx):\n        img = Image.open(self.paths[idx]).convert(\"RGB\")\n        img = self.transforms(img)\n        img = np.array(img)\n        img_lab = rgb2lab(img).astype(\"float32\") # Converting RGB to L*a*b\n        img_lab = transforms.ToTensor()(img_lab)\n        L = img_lab[[0], ...] / 50. - 1. # Between -1 and 1\n        ab = img_lab[[1, 2], ...] / 110. # Between -1 and 1\n        \n        return {'L': L, 'ab': ab}\n    \n    def __len__(self):\n        return len(self.paths)\n\ndef make_dataloaders(batch_size=16, n_workers=4, pin_memory=True, **kwargs): # A handy function to make our dataloaders\n    dataset = ColorizationDataset(**kwargs)\n    dataloader = DataLoader(dataset, batch_size=batch_size, num_workers=n_workers,\n                            pin_memory=pin_memory)\n    return dataloader","metadata":{"id":"x9W8F5IouEsk","execution":{"iopub.status.busy":"2022-06-19T11:53:14.466254Z","iopub.execute_input":"2022-06-19T11:53:14.466786Z","iopub.status.idle":"2022-06-19T11:53:14.480940Z","shell.execute_reply.started":"2022-06-19T11:53:14.466749Z","shell.execute_reply":"2022-06-19T11:53:14.479994Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"train_dl = make_dataloaders(paths=train_paths, split='train')\nval_dl = make_dataloaders(paths=val_paths, split='val')\n\ndata = next(iter(train_dl))\nLs, abs_ = data['L'], data['ab']\nprint(Ls.shape, abs_.shape)\nprint(len(train_dl), len(val_dl))","metadata":{"id":"NWSBStEnuIr2","outputId":"f014462f-4ba4-44fa-ec29-859c7ad848c6","execution":{"iopub.status.busy":"2022-06-19T11:53:14.483167Z","iopub.execute_input":"2022-06-19T11:53:14.483538Z","iopub.status.idle":"2022-06-19T11:53:17.367575Z","shell.execute_reply.started":"2022-06-19T11:53:14.483502Z","shell.execute_reply":"2022-06-19T11:53:17.365823Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"class UnetBlock(nn.Module):\n    def __init__(self, nf, ni, submodule=None, input_c=None, dropout=False,\n                 innermost=False, outermost=False):\n        super().__init__()\n        self.outermost = outermost\n        if input_c is None: input_c = nf\n        downconv = nn.Conv2d(input_c, ni, kernel_size=4,\n                             stride=2, padding=1, bias=False)\n        downrelu = nn.LeakyReLU(0.2, True)\n        downnorm = nn.BatchNorm2d(ni)\n        uprelu = nn.ReLU(True)\n        upnorm = nn.BatchNorm2d(nf)\n        \n        if outermost:\n            upconv = nn.ConvTranspose2d(ni * 2, nf, kernel_size=4,\n                                        stride=2, padding=1)\n            down = [downconv]\n            up = [uprelu, upconv, nn.Tanh()]\n            model = down + [submodule] + up\n        elif innermost:\n            upconv = nn.ConvTranspose2d(ni, nf, kernel_size=4,\n                                        stride=2, padding=1, bias=False)\n            down = [downrelu, downconv]\n            up = [uprelu, upconv, upnorm]\n            model = down + up\n        else:\n            upconv = nn.ConvTranspose2d(ni * 2, nf, kernel_size=4,\n                                        stride=2, padding=1, bias=False)\n            down = [downrelu, downconv, downnorm]\n            up = [uprelu, upconv, upnorm]\n            if dropout: up += [nn.Dropout(0.5)]\n            model = down + [submodule] + up\n        self.model = nn.Sequential(*model)\n    \n    def forward(self, x):\n        if self.outermost:\n            return self.model(x)\n        else:\n            return torch.cat([x, self.model(x)], 1)\n\nclass Unet(nn.Module):\n    def __init__(self, input_c=1, output_c=2, n_down=8, num_filters=64):\n        super().__init__()\n        unet_block = UnetBlock(num_filters * 8, num_filters * 8, innermost=True)\n        for _ in range(n_down - 5):\n            unet_block = UnetBlock(num_filters * 8, num_filters * 8, submodule=unet_block, dropout=True)\n        out_filters = num_filters * 8\n        for _ in range(3):\n            unet_block = UnetBlock(out_filters // 2, out_filters, submodule=unet_block)\n            out_filters //= 2\n        self.model = UnetBlock(output_c, out_filters, input_c=input_c, submodule=unet_block, outermost=True)\n    \n    def forward(self, x):\n        return self.model(x)\n    \n    \ndef build_res_unet(n_input=1, n_output=2, size=256):\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    body = create_body(resnet18, pretrained=True, n_in=n_input, cut=-2)\n    net_G = DynamicUnet(body, n_output, (size, size)).to(device)\n    return net_G\n","metadata":{"id":"21VPAlOFvBw7","execution":{"iopub.status.busy":"2022-06-19T11:53:17.369546Z","iopub.execute_input":"2022-06-19T11:53:17.369973Z","iopub.status.idle":"2022-06-19T11:53:17.388628Z","shell.execute_reply.started":"2022-06-19T11:53:17.369932Z","shell.execute_reply":"2022-06-19T11:53:17.387062Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"class PatchDiscriminator(nn.Module):\n    def __init__(self, input_c, num_filters=64, n_down=3):\n        super().__init__()\n        model = [self.get_layers(input_c, num_filters, norm=False)]\n        model += [self.get_layers(num_filters * 2 ** i, num_filters * 2 ** (i + 1), s=1 if i == (n_down-1) else 2) \n                          for i in range(n_down)] \n        \n        model += [self.get_layers(num_filters * 2 ** n_down, 1, s=1, norm=False, act=False)]\n        self.model = nn.Sequential(*model)                                                   \n        \n    def get_layers(self, ni, nf, k=4, s=2, p=1, norm=True, act=True): \n        layers = [nn.Conv2d(ni, nf, k, s, p, bias=not norm)]         \n        if norm: layers += [nn.BatchNorm2d(nf)]\n        if act: layers += [nn.LeakyReLU(0.2, True)]\n        return nn.Sequential(*layers)\n    \n    def forward(self, x):\n        return self.model(x)","metadata":{"id":"to4CMz5EvQw1","execution":{"iopub.status.busy":"2022-06-19T11:53:17.390208Z","iopub.execute_input":"2022-06-19T11:53:17.391011Z","iopub.status.idle":"2022-06-19T11:53:17.402087Z","shell.execute_reply.started":"2022-06-19T11:53:17.390968Z","shell.execute_reply":"2022-06-19T11:53:17.401252Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"PatchDiscriminator(3)","metadata":{"id":"PU6y9ccvvVBf","outputId":"f03a81dc-a78f-45d5-f0da-d34bbdad8863","execution":{"iopub.status.busy":"2022-06-19T11:53:17.404485Z","iopub.execute_input":"2022-06-19T11:53:17.405138Z","iopub.status.idle":"2022-06-19T11:53:17.439309Z","shell.execute_reply.started":"2022-06-19T11:53:17.405102Z","shell.execute_reply":"2022-06-19T11:53:17.438578Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"discriminator = PatchDiscriminator(3)\ndummy_input = torch.randn(16, 3, 256, 256)\nout = discriminator(dummy_input)\nout.shape","metadata":{"id":"eHN-B86ivX6k","outputId":"3549c74a-6248-4417-c8c3-823078c7d3a5","execution":{"iopub.status.busy":"2022-06-19T11:53:17.440535Z","iopub.execute_input":"2022-06-19T11:53:17.440869Z","iopub.status.idle":"2022-06-19T11:53:19.552353Z","shell.execute_reply.started":"2022-06-19T11:53:17.440844Z","shell.execute_reply":"2022-06-19T11:53:19.551601Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"class GANLoss(nn.Module):\n    def __init__(self, gan_mode='vanilla', real_label=1.0, fake_label=0.0):\n        super().__init__()\n        self.register_buffer('real_label', torch.tensor(real_label))\n        self.register_buffer('fake_label', torch.tensor(fake_label))\n        if gan_mode == 'vanilla':\n            self.loss = nn.BCEWithLogitsLoss()\n        elif gan_mode == 'lsgan':\n            self.loss = nn.MSELoss()\n    \n    def get_labels(self, preds, target_is_real):\n        if target_is_real:\n            labels = self.real_label\n        else:\n            labels = self.fake_label\n        return labels.expand_as(preds)\n    \n    def __call__(self, preds, target_is_real):\n        labels = self.get_labels(preds, target_is_real)\n        loss = self.loss(preds, labels)\n        return loss","metadata":{"id":"mwyGCYXWveL2","execution":{"iopub.status.busy":"2022-06-19T11:53:19.555389Z","iopub.execute_input":"2022-06-19T11:53:19.555938Z","iopub.status.idle":"2022-06-19T11:53:19.565025Z","shell.execute_reply.started":"2022-06-19T11:53:19.555899Z","shell.execute_reply":"2022-06-19T11:53:19.564330Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"def init_weights(net, init='norm', gain=0.02):\n    \n    def init_func(m):\n        classname = m.__class__.__name__\n        if hasattr(m, 'weight') and 'Conv' in classname:\n            if init == 'norm':\n                nn.init.normal_(m.weight.data, mean=0.0, std=gain)\n            elif init == 'xavier':\n                nn.init.xavier_normal_(m.weight.data, gain=gain)\n            elif init == 'kaiming':\n                nn.init.kaiming_normal_(m.weight.data, a=0, mode='fan_in')\n            \n            if hasattr(m, 'bias') and m.bias is not None:\n                nn.init.constant_(m.bias.data, 0.0)\n        elif 'BatchNorm2d' in classname:\n            nn.init.normal_(m.weight.data, 1., gain)\n            nn.init.constant_(m.bias.data, 0.)\n            \n    net.apply(init_func)\n    print(f\"model initialized with {init} initialization\")\n    return net\n\ndef init_model(model, device):\n    model = model.to(device)\n    model = init_weights(model)\n    return model","metadata":{"id":"LNrooEyeviTI","execution":{"iopub.status.busy":"2022-06-19T11:53:19.567496Z","iopub.execute_input":"2022-06-19T11:53:19.568008Z","iopub.status.idle":"2022-06-19T11:53:19.581245Z","shell.execute_reply.started":"2022-06-19T11:53:19.567971Z","shell.execute_reply":"2022-06-19T11:53:19.580459Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"class MainModel(nn.Module):\n    def __init__(self, net_G=None, lr_G=2e-4, lr_D=2e-4, \n                 beta1=0.5, beta2=0.999, lambda_L1=100.):\n        super().__init__()\n        \n        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        self.lambda_L1 = lambda_L1\n        \n        if net_G is None:\n            self.net_G = init_model(Unet(input_c=1, output_c=2, n_down=8, num_filters=64), self.device)\n        else:\n            self.net_G = net_G.to(self.device)\n        self.net_D = init_model(PatchDiscriminator(input_c=3, n_down=3, num_filters=64), self.device)\n        self.GANcriterion = GANLoss(gan_mode='vanilla').to(self.device)\n        self.L1criterion = nn.L1Loss()\n        self.opt_G = optim.Adam(self.net_G.parameters(), lr=lr_G, betas=(beta1, beta2))\n        self.opt_D = optim.Adam(self.net_D.parameters(), lr=lr_D, betas=(beta1, beta2))\n    \n    def set_requires_grad(self, model, requires_grad=True):\n        for p in model.parameters():\n            p.requires_grad = requires_grad\n        \n    def setup_input(self, data):\n        self.L = data['L'].to(self.device)\n        self.ab = data['ab'].to(self.device)\n        \n    def forward(self):\n        self.fake_color = self.net_G(self.L)\n    \n    def backward_D(self):\n        fake_image = torch.cat([self.L, self.fake_color], dim=1)\n        fake_preds = self.net_D(fake_image.detach())\n        self.loss_D_fake = self.GANcriterion(fake_preds, False)\n        real_image = torch.cat([self.L, self.ab], dim=1)\n        real_preds = self.net_D(real_image)\n        self.loss_D_real = self.GANcriterion(real_preds, True)\n        self.loss_D = (self.loss_D_fake + self.loss_D_real) * 0.5\n        self.loss_D.backward()\n    \n    def backward_G(self):\n        fake_image = torch.cat([self.L, self.fake_color], dim=1)\n        fake_preds = self.net_D(fake_image)\n        self.loss_G_GAN = self.GANcriterion(fake_preds, True)\n        self.loss_G_L1 = self.L1criterion(self.fake_color, self.ab) * self.lambda_L1\n        self.loss_G = self.loss_G_GAN + self.loss_G_L1\n        self.loss_G.backward()\n    \n    def optimize(self):\n        self.forward()\n        self.net_D.train()\n        self.set_requires_grad(self.net_D, True)\n        self.opt_D.zero_grad()\n        self.backward_D()\n        self.opt_D.step()\n        \n        self.net_G.train()\n        self.set_requires_grad(self.net_D, False)\n        self.opt_G.zero_grad()\n        self.backward_G()\n        self.opt_G.step()","metadata":{"id":"ViY7_IzNvyUa","execution":{"iopub.status.busy":"2022-06-19T11:53:19.582516Z","iopub.execute_input":"2022-06-19T11:53:19.583068Z","iopub.status.idle":"2022-06-19T11:53:19.607516Z","shell.execute_reply.started":"2022-06-19T11:53:19.583031Z","shell.execute_reply":"2022-06-19T11:53:19.606807Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"class AverageMeter:\n    def __init__(self):\n        self.reset()\n        \n    def reset(self):\n        self.count, self.avg, self.sum = [0.] * 3\n    \n    def update(self, val, count=1):\n        self.count += count\n        self.sum += count * val\n        self.avg = self.sum / self.count\n\ndef create_loss_meters():\n    loss_D_fake = AverageMeter()\n    loss_D_real = AverageMeter()\n    loss_D = AverageMeter()\n    loss_G_GAN = AverageMeter()\n    loss_G_L1 = AverageMeter()\n    loss_G = AverageMeter()\n    \n    return {'loss_D_fake': loss_D_fake,\n            'loss_D_real': loss_D_real,\n            'loss_D': loss_D,\n            'loss_G_GAN': loss_G_GAN,\n            'loss_G_L1': loss_G_L1,\n            'loss_G': loss_G}\n\ndef update_losses(model, loss_meter_dict, count):\n    for loss_name, loss_meter in loss_meter_dict.items():\n        loss = getattr(model, loss_name)\n        loss_meter.update(loss.item(), count=count)\n\ndef lab_to_rgb(L, ab): \n    L = (L + 1.) * 50.\n    ab = ab * 110.\n    Lab = torch.cat([L, ab], dim=1).permute(0, 2, 3, 1).cpu().numpy()\n    rgb_imgs = []\n    for img in Lab:\n        img_rgb = lab2rgb(img)\n        rgb_imgs.append(img_rgb)\n    return np.stack(rgb_imgs, axis=0)\n    \ndef visualize(model, data, save=True):\n    model.net_G.eval()\n    with torch.no_grad():\n        model.setup_input(data)\n        model.forward()\n    model.net_G.train()\n    fake_color = model.fake_color.detach()\n    real_color = model.ab\n    L = model.L\n    fake_imgs = lab_to_rgb(L, fake_color)\n    real_imgs = lab_to_rgb(L, real_color)\n    fig = plt.figure(figsize=(15, 8))\n    for i in range(5):\n        ax = plt.subplot(3, 5, i + 1)\n        ax.imshow(L[i][0].cpu(), cmap='gray')\n        ax.axis(\"off\")\n        ax = plt.subplot(3, 5, i + 1 + 5)\n        ax.imshow(fake_imgs[i])\n        ax.axis(\"off\")\n        ax = plt.subplot(3, 5, i + 1 + 10)\n        ax.imshow(real_imgs[i])\n        ax.axis(\"off\")\n    plt.show()\n    if save:\n        fig.savefig(f\"colorization_{time.time()}.png\")\n        \ndef log_results(loss_meter_dict):\n    for loss_name, loss_meter in loss_meter_dict.items():\n        print(f\"{loss_name}: {loss_meter.avg:.5f}\")","metadata":{"id":"3O2ieD2Nv34P","execution":{"iopub.status.busy":"2022-06-19T11:53:19.608811Z","iopub.execute_input":"2022-06-19T11:53:19.609393Z","iopub.status.idle":"2022-06-19T11:53:19.632246Z","shell.execute_reply.started":"2022-06-19T11:53:19.609358Z","shell.execute_reply":"2022-06-19T11:53:19.631472Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"def pretrain_generator(net_G, train_dl, opt, criterion, epochs):\n    for e in range(epochs):\n        loss_meter = AverageMeter()\n        for data in tqdm(train_dl):\n            L, ab = data['L'].to(device), data['ab'].to(device)\n            preds = net_G(L)\n            loss = criterion(preds, ab)\n            opt.zero_grad()\n            loss.backward()\n            opt.step()\n            \n            loss_meter.update(loss.item(), L.size(0))\n            \n        print(f\"Epoch {e + 1}/{epochs}\")\n        print(f\"L1 Loss: {loss_meter.avg:.5f}\")\n\nnet_G = build_res_unet(n_input=1, n_output=2, size=256)\nopt = optim.Adam(net_G.parameters(), lr=1e-4)\ncriterion = nn.L1Loss()        \npretrain_generator(net_G, train_dl, opt, criterion, 20)\ntorch.save(net_G.state_dict(), \"res18-unet.pt\")\n\n\ndef train_model(model, train_dl, epochs, display_every=200):\n    data = next(iter(val_dl)) \n    for e in range(epochs):\n        loss_meter_dict = create_loss_meters() \n        i = 0                                  \n        for data in tqdm(train_dl):\n            model.setup_input(data) \n            model.optimize()\n            update_losses(model, loss_meter_dict, count=data['L'].size(0))\n            i += 1\n            if i % display_every == 0:\n                print(f\"\\nEpoch {e+1}/{epochs}\")\n                print(f\"Iteration {i}/{len(train_dl)}\")\n                log_results(loss_meter_dict) \n                visualize(model, data, save=False) \n\nnet_G = build_res_unet(n_input=1, n_output=2, size=256)\nnet_G.load_state_dict(torch.load(\"res18-unet.pt\", map_location=device))\nmodel = MainModel(net_G=net_G)\n\ntrain_model(model, train_dl, 50)","metadata":{"id":"8gIPdiYxv8m2","outputId":"881867f8-cbf6-4efd-b954-694a042ae572","execution":{"iopub.status.busy":"2022-06-19T11:53:19.633612Z","iopub.execute_input":"2022-06-19T11:53:19.634110Z"},"trusted":true},"execution_count":null,"outputs":[]}]}